# config.yaml
# support multiple models and configurations

# default config name
default_config: llama-2-7b

# GPU config
gpu_config:
  cuda_visible_devices: "6"

# model config list
# Add new models here
models:
  # Llama-2-7b vanilla mode
  llama-2-7b:
    model_name_or_path: "/mnt/public/models/Llama-2-7b-hf"
    use_which_plan: vanilla
    output_layer: 27
    tp_starting_index: 0  # not used in vanilla mode
    tp_exiting_index: 0   # not used in vanilla mode
    batch_size: 16
    mode: test
    task_set: sts
    prompt_method: prompteol
    attention_enhance:
      enabled: true
      enable_attention_override: true
      head_order: score
      override_mode: scale_max
      score_file: "./head_score/llama-2-7b-80k.json"
      top_k: 25
      gamma: 1.2
      target_phrase: "means in one word:\""
      analysis_samples: 3
      analysis_dir: "attention_analysis"
      analysis_tasks:
        enable_heatmap_csv: true
        enable_prev_token_probe: true
      ablation_tasks:
        zero_means_attention_to_text: false

  llama-2-7b-attention:
    model_name_or_path: "/mnt/public/models/Llama-2-7b-hf"
    use_which_plan: vanilla
    output_layer: 27
    tp_starting_index: 0  # not used in vanilla mode
    tp_exiting_index: 0   # not used in vanilla mode
    batch_size: 16
    mode: test
    task_set: sts
    prompt_method: prompteol
    attention_enhance:
      enabled: true
      enable_attention_override: true
      head_order: score
      override_mode: zero_special
      zero_special_skip_threshold: 0.20
      zero_special_target_threshold: 0.99
      score_file: "./head_score/llama-2-7b-80k.json"
      top_k: 25
      gamma: 20
      target_phrase: "means in one word:\""
      analysis_samples: 3
      analysis_dir: "attention_analysis"
      analysis_tasks:
        enable_heatmap_csv: true
        enable_prev_token_probe: true
      ablation_tasks:
        zero_means_attention_to_text: false

  llama-2-7b-tp-attention:
    model_name_or_path: "/mnt/public/models/Llama-2-7b-hf"
    use_which_plan: tp
    output_layer: 27
    tp_starting_index: 1  # not used in vanilla mode
    tp_exiting_index: 7   # not used in vanilla mode
    batch_size: 16
    mode: dev
    task_set: sts
    prompt_method: prompteol
    attention_enhance:
      enabled: true
      enable_attention_override: true
      head_order: score
      override_mode: scale_max
      score_file: "./head_score/llama-2-7b-80k.json"
      top_k: 25
      gamma: 15
      target_phrase: "means in one word:\""
      analysis_samples: 0
      analysis_dir: "attention_analysis"
      analysis_tasks:
        enable_heatmap_csv: true
        enable_prev_token_probe: true
      ablation_tasks:
        zero_means_attention_to_text: true

  # Llama-2-7b TP mode
  llama-2-7b-tp:
    model_name_or_path: "/tos-bjml-ai4chem/fuyuchen/llm_weights/Llama-2-7b-hf"
    use_which_plan: tp
    output_layer: 27
    tp_starting_index: 1
    tp_exiting_index: 7
    batch_size: 16
    mode: test
    task_set: sts
    prompt_method: prompteol
    attention_enhance:
      enabled: false
      enable_attention_override: true
      head_order: score
      override_mode: scale_max
      score_file: "./head_score/llama-2-7b-80k.json"
      top_k: 10
      gamma: 1.2
      target_phrase: "means in one word:"
      analysis_samples: 3
      analysis_dir: "attention_analysis"
      analysis_tasks:
        enable_heatmap_csv: true

  llama-2-13b:
    model_name_or_path: "/tos-bjml-ai4chem/fuyuchen/llm_weights/Llama-2-7b-hf"
    use_which_plan: vanilla
    output_layer: -1
    tp_starting_index: 0  
    tp_exiting_index: 0   
    batch_size: 16
    mode: test
    task_set: sts
    prompt_method: cot
    attention_enhance:
      enabled: false
      enable_attention_override: true
      head_order: score
      override_mode: scale_max
      score_file: "./head_score/llama-2-7b-80k.json"
      top_k: 10
      gamma: 1.2
      target_phrase: "means in one word:"
      analysis_samples: 3
      analysis_dir: "attention_analysis"
      analysis_tasks:
        enable_heatmap_csv: true

  llama-2-13b-tp:
    model_name_or_path: "/tos-bjml-ai4chem/fuyuchen/llm_weights/Llama-2-7b-hf"
    use_which_plan: tp
    output_layer: -2
    tp_starting_index: 1
    tp_exiting_index: 7
    batch_size: 16
    mode: test
    task_set: sts
    prompt_method: cot
    attention_enhance:
      enabled: false
      enable_attention_override: true
      head_order: score
      override_mode: scale_max
      score_file: "./head_score/llama-2-7b-80k.json"
      top_k: 10
      gamma: 1.2
      target_phrase: "means in one word:"
      analysis_samples: 3
      analysis_tasks:
        enable_heatmap_csv: true

  llama-3-8b:
    model_name_or_path: "/tos-bjml-ai4chem/fuyuchen/llm_weights/Llama-2-7b-hf"
    use_which_plan: vanilla
    output_layer: -1
    tp_starting_index: 0  
    tp_exiting_index: 0   
    batch_size: 16
    mode: test
    task_set: sts
    prompt_method: cot

  llama-3-8b-tp:
    model_name_or_path: "/tos-bjml-ai4chem/fuyuchen/llm_weights/Llama-2-7b-hf"
    use_which_plan: tp
    output_layer: -2
    tp_starting_index: 1
    tp_exiting_index: 3
    batch_size: 16
    mode: test
    task_set: sts
    prompt_method: cot

  qwen2-7b:
    model_name_or_path: "/path/to/qwen2-7b"
    use_which_plan: vanilla
    output_layer: -1
    tp_starting_index: 0  
    tp_exiting_index: 0   
    batch_size: 16
    mode: test
    task_set: sts
    prompt_method: cot

  qwen2-7b-tp:
    model_name_or_path: "/path/to/qwen2-7b"
    use_which_plan: tp
    output_layer: -2
    tp_starting_index: 1
    tp_exiting_index: 6
    batch_size: 16
    mode: test
    task_set: sts
    prompt_method: cot

  gemma2-9b:
    model_name_or_path: "/tos-bjml-ai4chem/fuyuchen/llm_weights/gemma-2-9b"
    use_which_plan: vanilla
    output_layer: -1
    tp_starting_index: 0  
    tp_exiting_index: 0   
    batch_size: 16
    mode: test
    task_set: sts
    prompt_method: cot

  gemma2-9b-tp:
    model_name_or_path: "/tos-bjml-ai4chem/fuyuchen/llm_weights/gemma-2-9b"
    use_which_plan: tp
    output_layer: -2
    tp_starting_index: 1
    tp_exiting_index: 6
    batch_size: 16
    mode: test
    task_set: sts
    prompt_method: cot
